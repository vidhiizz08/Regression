{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions"
      ],
      "metadata": {
        "id": "eVSE8q9HDnGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is Simple Linear Regression?**\n",
        "   - Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and an independent variable (X) using a straight-line equation: **Y = mX + c**.\n",
        "\n",
        "2. **What are the key assumptions of Simple Linear Regression?**\n",
        "   1. Linearity: The relationship between X and Y is linear.\n",
        "   2. Independence: Observations are independent of each other.\n",
        "   3. Homoscedasticity: Constant variance of residuals.\n",
        "   4. Normality: Residuals follow a normal distribution.\n",
        "   5. No or minimal multicollinearity.\n",
        "\n",
        "3. **What does the coefficient m represent in the equation Y = mX + c?**\n",
        "   - The coefficient **m** (slope) represents the rate of change in Y for a one-unit change in X. It quantifies the strength and direction of the relationship.\n",
        "\n",
        "4. **What does the intercept c represent in the equation Y = mX + c?**\n",
        "   - The intercept **c** is the value of Y when X is zero. It represents the starting point of the regression line.\n",
        "\n",
        "5. **How do we calculate the slope m in Simple Linear Regression?**\n",
        "   - The slope **m** is calculated using the formula: ![m = (Σ(X - X̄)(Y - Ȳ)) / (Σ(X - X̄)²)] where X̄ and Ȳ are the mean values of X and Y.\n",
        "\n",
        "6. **What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "   - The least squares method minimizes the sum of squared residuals (differences between actual and predicted values) to find the best-fitting regression line.\n",
        "\n",
        "7. **How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "   - R² measures the proportion of variance in the dependent variable explained by the independent variable. An R² value close to 1 indicates a strong relationship.\n",
        "\n",
        "8. **What is Multiple Linear Regression?**\n",
        "   - Multiple Linear Regression models the relationship between a dependent variable and multiple independent variables using the equation: **Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ**.\n",
        "\n",
        "9. **What is the main difference between Simple and Multiple Linear Regression?**\n",
        "   - Simple Linear Regression has one independent variable, while Multiple Linear Regression has two or more independent variables.\n",
        "\n",
        "10. **What are the key assumptions of Multiple Linear Regression?**\n",
        "    1. Linearity\n",
        "    2. Independence of errors\n",
        "    3. Homoscedasticity\n",
        "    4. No perfect multicollinearity\n",
        "    5. Normality of residuals\n",
        "\n",
        "11. **What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "    - Heteroscedasticity occurs when the variance of residuals is not constant. It can lead to inefficient estimates and affect hypothesis testing.\n",
        "\n",
        "12. **How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "    1. Remove highly correlated variables\n",
        "    2. Use Principal Component Analysis (PCA)\n",
        "    3. Apply Ridge or Lasso regression\n",
        "    4. Collect more data\n",
        "\n",
        "13. **What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "    1. One-hot encoding\n",
        "    2. Label encoding\n",
        "    3. Ordinal encoding\n",
        "    4. Dummy variables\n",
        "\n",
        "14. **What is the role of interaction terms in Multiple Linear Regression?**\n",
        "    - Interaction terms capture the combined effect of two or more variables when their effect is not purely additive.\n",
        "\n",
        "15. **How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "   - In Simple Linear Regression, the intercept represents the expected value of Y when X = 0. In Multiple Linear Regression, it represents the expected value of Y when all independent variables are zero.\n",
        "\n",
        "16. **What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "    - The slope represents the change in the dependent variable for a one-unit change in the independent variable, helping in making predictions.\n",
        "\n",
        "17. **How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "    - The intercept provides a baseline value for Y when all independent variables are zero. However, its practical interpretation depends on the dataset.\n",
        "\n",
        "18. **What are the limitations of using R² as a sole measure of model performance?**\n",
        "    - R² does not indicate whether the model is correctly specified, and it can be artificially high due to overfitting. Adjusted R² is often preferred.\n",
        "\n",
        "19. **How would you interpret a large standard error for a regression coefficient?**\n",
        "    - A large standard error suggests high variability in the coefficient estimate, indicating that the predictor may not be reliable.\n",
        "\n",
        "20. **How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "    - Heteroscedasticity appears as a funnel shape in residual plots. Addressing it ensures valid hypothesis testing and better model efficiency.\n",
        "\n",
        "21. **What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "    - It suggests that some predictors do not contribute significantly, leading to overfitting. Adjusted R² accounts for the number of predictors.\n",
        "\n",
        "22. **Why is it important to scale variables in Multiple Linear Regression?**\n",
        "    - Scaling ensures that variables with different units and magnitudes do not disproportionately affect the model, improving convergence and stability.\n",
        "\n",
        "23. **What is polynomial regression?**\n",
        "    - Polynomial regression extends linear regression by fitting a polynomial equation to the data, capturing non-linear relationships.\n",
        "\n",
        "24. **How does polynomial regression differ from linear regression?**\n",
        "    - Polynomial regression allows for curved relationships by adding higher-degree terms, while linear regression models only straight-line relationships.\n",
        "\n",
        "25. **When is polynomial regression used?**\n",
        "    - When data exhibits a non-linear relationship that cannot be well-represented by a simple straight line.\n",
        "\n",
        "26. **What is the general equation for polynomial regression?**\n",
        "    - Y = b₀ + b₁X + b₂X² + ... + bₙXⁿ, where higher-degree terms capture non-linearity.\n",
        "\n",
        "27. **Can polynomial regression be applied to multiple variables?**\n",
        "    - Yes, polynomial regression can extend to multiple variables, creating polynomial terms for each predictor.\n",
        "\n",
        "28. **What are the limitations of polynomial regression?**\n",
        "    1. Risk of overfitting\n",
        "    2. Higher complexity\n",
        "    3. Sensitive to outliers\n",
        "    4. Increased computational cost\n",
        "\n",
        "29. **What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "    1. Cross-validation\n",
        "    2. Adjusted R²\n",
        "    3. AIC/BIC\n",
        "    4. Residual analysis\n",
        "\n",
        "30. **Why is visualization important in polynomial regression?**\n",
        "    - Visualization helps in assessing the fit of the polynomial curve and identifying overfitting or underfitting.\n",
        "\n",
        "31. **How is polynomial regression implemented in Python?**\n",
        "    - Polynomial regression can be implemented using **scikit-learn**:\n",
        "    ```python\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    poly = PolynomialFeatures(degree=2)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "    ```\n",
        "\n"
      ],
      "metadata": {
        "id": "h1JJJqttDzN2"
      }
    }
  ]
}